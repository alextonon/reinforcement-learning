




















import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt





import gymnasium as gym
cartpole = gym.make('CartPole-v1', render_mode='human')








cartpole._max_episode_steps





print(cartpole.action_space)
print(cartpole.observation_space)
print(cartpole.env.metadata)


import time
import numpy as np

_ = cartpole.reset()
cartpole.render()
for i in range(10000):
    _, _, d, _, _ = cartpole.step(np.random.randint(2))
    cartpole.render()
    if d:
        print("Final nb steps:", i)
        break
    time.sleep(cartpole.unwrapped.tau) # not actually real-time, just for readability


cartpole.close()





from environments.swingup import CartPoleSwingUp
swingup = CartPoleSwingUp(render_mode="human")


import numpy as np

x = swingup.reset()
swingup.render()

for i in range(1000):
    _, _, d, _, _ = swingup.step(np.random.randint(2))
    swingup.render()
    if d:
        print("Final nb steps:", i)
        break
    time.sleep(cartpole.unwrapped.tau)


swingup.close()





import gymnasium as gym
import ale_py

gym.register_envs(ale_py)
pong = gym.make('ALE/Pong-v5')





import numpy as np
print(pong.observation_space)
print(pong.observation_space.shape)
print(np.min(pong.observation_space.low))
print(np.max(pong.observation_space.high))
print(pong.action_space)
#help(env.observation_space)


%matplotlib inline
import matplotlib.pyplot as plt
s, _ = pong.reset()
plt.imshow(s)
plt.show()











from environments.pongwrapper import PongWrapper


pong = PongWrapper(
    render_mode="human",
    noop_max=0,
    frame_skip=4,
    terminal_on_life_loss=True,
    grayscale_obs=True,
    scale_obs=True,
)


# Trying a random agent in Pong
import time
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt

pong.reset()
for i in range(60):
    a = np.random.randint(2)
    s, r, _, _, _ = pong.step(a)
    time.sleep(1 / 60)
    
pong.close()
print("shape: ", s.shape, ", min = ", s.min(), ", max = ", s.max(), ", dtype = ", s.dtype, sep='')
plt.imshow(s, cmap='gray');


pong = PongWrapper(
    noop_max=0,
    frame_skip=4,
    terminal_on_life_loss=True,
    grayscale_obs=True,
    scale_obs=True,
)


























# %load solutions/RL5_exercise1.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).

# Replay buffer class
import random
from tqdm import trange

class ReplayBuffer:
    def __init__(self, capacity):
        self.data = ...
        ...

    def append(self, s, a, r, s_, d):
        ...

    def sample(self, batch_size):
        ...

    def __len__(self):
        return len(self.data)


cartpole = gym.make("CartPole-v1")

# Testing insertion in the ReplayBuffer class
replay_buffer_size = int(1e6)
nb_samples = int(2e6)

memory = ReplayBuffer(replay_buffer_size)
state, _ = cartpole.reset()
for _ in trange(nb_samples):
    action = cartpole.action_space.sample()
    next_state, reward, done, _, _ = cartpole.step(action)
    memory.append(state, action, reward, next_state, done)
    if done:
        state, _ = cartpole.reset()
    else:
        state = next_state

print(len(memory))

# Testing sampling in the ReplayBuffer class
nb_batches = int(1e4)
batch_size = 50

for _ in trange(nb_batches):
    batch = memory.sample(batch_size)

print(memory.sample(2))











# %load solutions/RL5_exercise2.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).

import torch
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

state_dim = ...
n_action = ...
nb_neurons = 24

DQN = ...














# %load solutions/RL5_exercise3.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).

import random
import torch

class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity # capacity of the buffer
        self.data = []
        self.index = 0 # index of the next cell to be filled

    def append(self, s, a, r, s_, d):
        if len(self.data) < self.capacity:
            self.data.append(None)
        self.data[self.index] = (s, a, r, s_, d)
        self.index = (self.index + 1) % self.capacity

    def sample(self, batch_size):
        ...

    def __len__(self):
        return len(self.data)





# %load solutions/RL5_exercise4.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).

import torch

def greedy_action(network, state):
    ...





# %load solutions/RL5_exercise5.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).

import numpy as np
import torch

class DQNAgent:
    def __init__(self, config, model):
        self.gamma = config["gamma"]
        self.batch_size = config["batch_size"]
        self.nb_actions = config["nb_actions"]
        self.memory = ReplayBuffer(config["buffer_size"])
        self.epsilon_max = config["epsilon_max"]
        self.epsilon_min = config["epsilon_min"]
        self.epsilon_stop = config["epsilon_decay_period"]
        self.epsilon_delay = config["epsilon_delay_decay"]
        self.epsilon_step = (self.epsilon_max - self.epsilon_min) / self.epsilon_stop
        self.total_steps = 0
        self.model = model
        self.criterion = torch.nn.MSELoss()
        self.optimizer = torch.optim.RMSprop(
            self.model.parameters(), lr=config["learning_rate"],
        )

    def gradient_step(self):
        if len(self.memory) < self.batch_size:
            return

        # Sample from memory
        ...

        # Compute y_t = r(s, a) + \gamma * max_a' Q(s', a')
        ...

        # Compute loss
        ...

        # Gradient step
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def train(self, env, max_episode):
        episode_return = []
        episode = 0
        episode_cum_reward = 0
        state = env.reset()
        epsilon = self.epsilon_max
        step = 0

        while episode < max_episode:
            # Update epsilon
            ...

            # Select epsilon-greedy action
            ...

            # Step and store in memory
            ...

            # Train
            self.gradient_step()

            # Next transition
            step += 1
            if done:
                episode += 1
                print(
                    f"Episode {episode:3d}, epsilon {epsilon:6.2f},",
                    f"batch size {len(self.memory):5d},",
                    f"episode return {episode_cum_reward:4.1f}",
                )
                state = env.reset()
                episode_return.append(episode_cum_reward)
                episode_cum_reward = 0
            else:
                state = next_state

        return episode_return






# %load solutions/RL5_exercise6.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).


cartpole = gym.make("CartPole-v1", render_mode="human")

s, _ = cartpole.reset()
for i in range(1000):
    a = greedy_action(DQN, s)
    s, _, d, _, _ = cartpole.step(a)
    if d:
        print("Final nb steps:", i)
        break

cartpole.close()




















# %load solutions/RL5_exercise7.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).





# %load solutions/RL5_exercise8.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).


cartpole = gym.make("CartPole-v1", render_mode="human")

DQN.load_state_dict(torch.load("cart_pole_dqn.pth"))
s, _ = cartpole.reset()
for i in range(1000):
    a = greedy_action(DQN, s)
    s, _, d, _, _ = cartpole.step(a)
    if d:
        print(i)
        break

cartpole.close()





# %load solutions/RL5_exercise9.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).





# %load solutions/RL5_exercise10.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).








# %load solutions/RL5_exercise11.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).


DQN.load_state_dict(torch.load("swingup_dqn.pth"))

swingup = CartPoleSwingUp(render_mode="human")
s, _ = swingup.reset()
tot_rew = 0
for _ in range(1000):
    a = greedy_action(DQN, s)
    s, r, d, _, _ = swingup.step(a)
    tot_rew += r
    if d:
        break

print(f"Total nb steps: {i}, total reward: {tot_rew}")

swingup.close()








from gymnasium.wrappers import FrameStackObservation
pong = FrameStackObservation(pong, 4)


s, _ = pong.reset()
print(torch.Tensor(s).shape)








# %load solutions/RL5_exercise12.py
### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).





from IPython.display import YouTubeVideo
YouTubeVideo("p88R2_3yWPA")


YouTubeVideo("TmPfTpjtdgg")











from environments.bicycle import Bicycle
bike = Bicycle()
s, _ = bike.reset()
print(s)





import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt

print(bike.action_set)
S = []
s, _ = bike.reset()
S.append(s)
for i in range(1000):
    s, r, d, _, _ = bike.step(0)
    S.append(s)
    if d:
        break

print("trajectory length", len(S))
S = np.array(S)
plt.plot(S[:, 4], S[:, 5]);





from environments.hiv_patient import HIVPatient
patient = HIVPatient()
print("Uninfected patient:", patient.reset("uninfected")[0])
print("Unhealthy patient:", patient.reset()[0]) # actually equivalent to patient.reset("unhealthy")
print("Healthy patient:", patient.reset(mode="healthy")[0])





%matplotlib inline
import matplotlib.pyplot as plt
from environments.hiv_patient import HIVPatient

patient = HIVPatient()
s, _ = patient.reset(mode="healthy")
s[5] *= .75
patient.E *= .75
print(patient.E)
dur = 80 # 400/5
#nb_steps = int(dur//1e-3)
states = [s]
for _ in range(dur):
    s, r, d, _, _ = patient.step(0)
    states.append(s)
    
fig, axs = plt.subplots(2, 3, figsize=(15, 15))
npst = np.array(states)
axs[0,0].plot(npst[:,0])
axs[0,0].set_title("T1")
axs[0,1].plot(npst[:,1])
axs[0,1].set_title("T1*")
axs[0,2].plot(npst[:,2])
axs[0,2].set_title("T2")
axs[1,0].plot(npst[:,3])
axs[1,0].set_title("T2*")
axs[1,1].plot(npst[:,4])
axs[1,1].set_title("V")
axs[1,2].plot(npst[:,5])
axs[1,2].set_title("E");





print(patient.action_set)






